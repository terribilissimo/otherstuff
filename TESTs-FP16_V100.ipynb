{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesla V100-DGXS-32Gb: test FP32 vs. FP16\n",
    "\n",
    "#### Pay attention: Same data for FP32 and FP16, but the batch size is doubled in the latter case.\n",
    "\n",
    "TL;DR version:\n",
    "\n",
    "FP32: bs=105, vram occupation peaks at `11623` Mb **(see NOTE)**, wall time: `1:04`, final val_loss `0.023873`\n",
    "\n",
    "FP16: bs=210, vram occupation peaks at `11782` Mb **(see NOTE)**, wall time: `0:51`, final val_loss `0.053197`\n",
    "\n",
    "*NOTE: `ipyexeriments` fails to correctly report vram occupation in the V100 case. Data about vram occupation were visually monitored with `gpustat`\n",
    "\n",
    "---\n",
    "The 1080ti achieved:\n",
    "\n",
    "FP32: bs=105, vram occupation peaks at `10074` Mb, wall time: `1:45`, final val_loss `0.020775`\n",
    "\n",
    "FP16: bs=210, vram occupation peaks at `10162` Mb, wall time: `1:37`, final val_loss `0.056734`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 3, Tesla V100-DGXS-32GB (32478 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:     Used     Free    Total        Util\n",
      "CPU:    2,447  230,583  257,868 MB   0.95% \n",
      "GPU:      953   31,524   32,478 MB   2.94% \n",
      "\n",
      "\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.000\n",
      "･ CPU:          0          0      2,448 MB |\n",
      "･ GPU:          0        491        953 MB |\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "import re\n",
    "import scipy.ndimage\n",
    "from ipyexperiments import *\n",
    "import fastai\n",
    "fastai.__version__\n",
    "\n",
    "torch.cuda.set_device(3)\n",
    "path    = Path('/raid/data/DATASET3/TESTVALID/')\n",
    "fp32exp = IPyExperimentsPytorch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.782\n",
      "･ CPU:          3          2      2,546 MB |\n",
      "･ GPU:          0        491        953 MB |\n"
     ]
    }
   ],
   "source": [
    "path = Path('/raid/data/DATASET3/TESTVALID/')\n",
    "bs=105\n",
    "data = ImageDataBunch.from_folder(path,\n",
    "                                  train='backup224',\n",
    "                                  valid='backup224valid',\n",
    "                                  size=224, bs=bs,\n",
    "                                  ).normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:01.287\n",
      "･ CPU:          0          0      2,658 MB |\n",
      "･ GPU:        106        385      1,059 MB |\n"
     ]
    }
   ],
   "source": [
    "learn = create_cnn(data, models.resnet50, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 01:04 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>1.088248</th>\n",
       "    <th>0.447044</th>\n",
       "    <th>0.864670</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>0.667350</th>\n",
       "    <th>0.104905</th>\n",
       "    <th>0.976876</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>0.402612</th>\n",
       "    <th>0.045042</th>\n",
       "    <th>0.991660</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>0.255537</th>\n",
       "    <th>0.027617</th>\n",
       "    <th>0.996588</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>0.171024</th>\n",
       "    <th>0.023873</th>\n",
       "    <th>0.997346</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.9 s, sys: 41.9 s, total: 1min 13s\n",
      "Wall time: 1min 4s\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:01:04.826\n",
      "･ CPU:          0          0      2,702 MB |\n",
      "･ GPU:        300         85      1,359 MB |\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "learn.fit_one_cycle(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Kernel is now restarted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Experiment started with the Pytorch backend\n",
      "Device: ID 3, Tesla V100-DGXS-32GB (32478 RAM)\n",
      "\n",
      "\n",
      "*** Current state:\n",
      "RAM:     Used     Free    Total        Util\n",
      "CPU:    2,419  230,608  257,868 MB   0.94% \n",
      "GPU:      953   31,524   32,478 MB   2.94% \n",
      "\n",
      "\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:00.000\n",
      "･ CPU:          0          0      2,421 MB |\n",
      "･ GPU:          0        491        953 MB |\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "import re\n",
    "import scipy.ndimage\n",
    "from ipyexperiments import *\n",
    "import fastai\n",
    "fastai.__version__\n",
    "\n",
    "torch.cuda.set_device(3)\n",
    "path    = Path('/raid/data/DATASET3/TESTVALID/')\n",
    "fp32exp = IPyExperimentsPytorch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:01.405\n",
      "･ CPU:          3          2      2,608 MB |\n",
      "･ GPU:          0        491        953 MB |\n"
     ]
    }
   ],
   "source": [
    "path = Path('/raid/data/DATASET3/TESTVALID/')\n",
    "bs=210\n",
    "data = ImageDataBunch.from_folder(path,\n",
    "                                  train='backup224',\n",
    "                                  valid='backup224valid',\n",
    "                                  size=224, bs=bs,\n",
    "                                  ).normalize(imagenet_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:01.144\n",
      "･ CPU:          0          0      2,639 MB |\n",
      "･ GPU:         82        409      1,035 MB |\n"
     ]
    }
   ],
   "source": [
    "learn = create_cnn(data, models.resnet50, metrics=accuracy).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 00:51 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>1.159473</th>\n",
       "    <th>0.532623</th>\n",
       "    <th>0.854435</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>0.839299</th>\n",
       "    <th>0.227999</th>\n",
       "    <th>0.952616</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>0.592292</th>\n",
       "    <th>0.089819</th>\n",
       "    <th>0.981425</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>0.429999</th>\n",
       "    <th>0.057339</th>\n",
       "    <th>0.990523</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>0.327299</th>\n",
       "    <th>0.053197</th>\n",
       "    <th>0.992419</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.3 s, sys: 32 s, total: 49.3 s\n",
      "Wall time: 51.8 s\n",
      "･ RAM:  △Consumed    △Peaked    Used Total | Exec time 0:00:51.770\n",
      "･ CPU:          0          0      2,684 MB |\n",
      "･ GPU:        662       -252      1,697 MB |\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "learn.fit_one_cycle(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
